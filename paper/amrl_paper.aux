\relax 
\citation{thrun2002robotic}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The ClearPath Jackal equipped with two fish-eye camera lenses.}}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Localization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Particle Filters}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.1}Elapse Time}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.2}Weigh}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.3}Resample}{2}}
\citation{deeplearningbook,AlexNet}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Convolutional Neural Networks}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A fully connected, feed-forward neural network. Neurons are connected to each neuron in the previous and next layers. Image source: http://neuralnetworksanddeeplearning.com/chap6.html}}{3}}
\citation{irie2010mobile}
\citation{Harris}
\citation{agrawal2006real}
\citation{Kalman}
\citation{valgren2007sift}
\citation{sift}
\citation{surf}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional neural network. The number of learned parameters is greatly reduced compared to fully connected architectures through the use of convolutional and pooling layers. Image source: http://colah.github.io/posts/2014-07-Conv-Nets-Modular/}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Related Work}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A picture of the robot used in the work of Agrawal et al., which employs stereo cameras with GPS for localization. Image source: http://www.ai.sri.com/\nobreakspace  {}agrawal/icpr06.pdf}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Point clouds generated by Irie et al. on their data set for localization using stereo cameras. Image source: $http://www.furo.org/irie/stereo_localization_iros10.pdf$}}{4}}
\citation{cobot}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of SIFT features generated for an image. Both SIFT and SURF generate feature vectors for images by finding key points such as edges, corners, or other areas where certain image gradients are higher. Image source: http://cs.brown.edu/courses/cs143/results/proj2/valayshah/}}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Methodology}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Data Set}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A satellite image of the map used in our data set. The red map shows the path traversed by the robot during data collection. Image source: https://www.google.com/maps}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Particle Filter Algorithm and Visualizer}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A visualization of the particle filter using GPS. The blue path denotes the set of all GPS readings while the yellow path denotes the readings already given to the robot. The red dots represent the particle distribution and the green line, stemming from the particle cloud, represents the robot's pose.}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Neural Perceptual Model}{5}}
\citation{AlexNet}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The AlexNet CNN architecture. At each layer features are extracted, starting from lower level features such as edges and colors, growing in abstraction to feature types such as partial objects. Image source: http://www.cc.gatech.edu/\nobreakspace  {}hays/compvision/proj6/}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Particle Filter}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A visualization of the particle filter using only wheel odometry with synthetic data denoting forward movement. As may be seen, without any correction from a perceptual model, the uncertainty of the model grows quickly over time. }}{6}}
\newlabel{odometry_error}{{10}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A visualization of the particle filter using wheel odometry and GPS with synthetic data denoting forward movement. It may be observed that the distribution of particles is maintained within the GPS radius of uncertainty. }}{6}}
\newlabel{with_gps}{{11}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Visual Classification}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A visualization of the classes generated on our dataset through K-Means clustering using cartesian coordinates in meters. Each colored patch denotes a different class, the average distance of GPS points in our data set to the nearest cluster center was 5.53 meters.}}{7}}
\newlabel{clustering}{{12}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The accuracy of the K-Nearest Neighbor classification on feature vectors pulled from a variety of layers of the AlexNet CNN. }}{7}}
\newlabel{classification_results}{{13}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Error Analysis}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Visualizations of error from the fifth pooling layer. \textit  {Top Left)} The continuous multi-colored path represents the ground truth path taken in the test run. The location of colored dots represent the location they were classified under, while the color corresponds to the true classification on the path. \textit  {Top Right)} The distance between labels assigned to images and the location of their true label. This graphic shows that the majority of error was kept within a 10 meter radius. \textit  {Bottom Left)} Another measure of error between assigned labels and their true class. From this it may be inferred that the greatest error occurred towards the beginning of the run and at the end. \textit  {Bottom Right)} The same measure as the bottom left, sorted by error magnitude. Like the top right, this graphic implies that the majority of error was kept within a 10 meter distance.}}{7}}
\newlabel{error_graphs}{{14}{7}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{agrawal2006real}{1}
\bibcite{surf}{2}
\bibcite{deeplearningbook}{3}
\bibcite{Harris}{4}
\bibcite{irie2010mobile}{5}
\bibcite{Kalman}{6}
\bibcite{AlexNet}{7}
\bibcite{sift}{8}
\bibcite{thrun2002robotic}{9}
\bibcite{valgren2007sift}{10}
\bibcite{cobot}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Visualizations of error from the fifth pooling layer. \textit  {Top Left)} \textit  {Top Right)} \textit  {Bottom Left)} \textit  {Bottom Right)}}}{8}}
\newlabel{confusion_matrix}{{15}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Discussion and Future Work}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}conclusion}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
