\relax 
\citation{thrun2002robotic}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The ClearPath Jackal equipped with two fish-eye camera lenses.}}{1}}
\citation{thrun2002particle}
\@writefile{toc}{\contentsline {section}{\numberline {II}Background}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Localization}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Particle Filters}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.1}Elapse Time}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.2}Weigh}{2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {II-B.3}Resample}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Convolutional Neural Networks}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A fully connected, feed-forward neural network. Neurons are connected to each neuron in the previous and next layers. Image source: http://neuralnetworksanddeeplearning.com/chap6.html}}{3}}
\citation{deeplearningbook,AlexNet}
\citation{laser_range}
\citation{irie2010mobile}
\citation{Harris}
\citation{agrawal2006real}
\citation{Kalman}
\citation{valgren2007sift}
\citation{sift}
\citation{surf}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional neural network. The number of learned parameters is greatly reduced compared to fully connected architectures through the use of convolutional and pooling layers. Image source: http://colah.github.io/posts/2014-07-Conv-Nets-Modular/}}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Related Work}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Point clouds generated by Irie et al. on their data set for localization using stereo cameras. Image source: $http://www.furo.org/irie/stereo_localization_iros10.pdf$}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A picture of the robot used in the work of Agrawal et al., which employs stereo cameras with GPS for localization. Image source: http://www.ai.sri.com/\nobreakspace  {}agrawal/icpr06.pdf}}{4}}
\citation{cobot}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of SIFT features generated for an image. Both SIFT and SURF generate feature vectors for images by finding key points such as edges, corners, or other areas where certain image gradients are higher. Image source: http://cs.brown.edu/courses/cs143/results/proj2/valayshah/}}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Methodology}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Data Set}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A satellite image of the map used in our data set. The red set of points shows the path traversed by the robot during data collection. Image source: https://www.google.com/maps}}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Particle Filter Algorithm and Visualizer}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Neural Perceptual Model}{5}}
\citation{AlexNet}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces A visualization of the particle filter generated using the Cobot visualizer. The blue path denotes the set of all GPS readings while the yellow path denotes the readings already given to the filter from a run data log file. The red dots represent the particle distribution and the green line, stemming from the particle cloud, represents the robot's pose estimate heading.}}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Results}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Particle Filter}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The AlexNet CNN architecture. Features are extracted at each layer, starting from lower level features such as edges and colors, growing in abstraction to feature types such as partial objects. Image source: http://www.cc.gatech.edu/\nobreakspace  {}hays/compvision/proj6/}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A visualization of the particle filter using only wheel odometry from synthetic data denoting forward movement. As may be seen, without any correction from a perceptual model, the pose estimate uncertainty quickly grows.}}{6}}
\newlabel{odometry_error}{{10}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces A visualization of the particle filter using wheel odometry and GPS with synthetic data denoting forward movement. It may be observed that the distribution of particles is maintained within the GPS radius of uncertainty. }}{7}}
\newlabel{with_gps}{{11}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Visual Classification}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A visualization of the classes generated on our dataset through K-Means clustering using cartesian coordinates in meters. Each colored patch denotes a different class, the average distance of GPS points in our data set to the nearest cluster center was 5.53 meters.}}{7}}
\newlabel{clustering}{{12}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The accuracy of the K-Nearest Neighbor classification on feature vectors pulled from a variety of layers of the AlexNet CNN. }}{7}}
\newlabel{classification_results}{{13}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Error Analysis}{7}}
\bibstyle{plain}
\bibdata{bibliography}
\bibcite{agrawal2006real}{1}
\bibcite{surf}{2}
\bibcite{deeplearningbook}{3}
\bibcite{Harris}{4}
\bibcite{irie2010mobile}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Visualizations of error from the fifth pooling layer. \textit  {Top Left)} The continuous multi-colored curve represents the ground truth image labels of the path taken in the test run. The locations of colored dots represent the location cluster labels images were given, while the color corresponds to the true label that should have been assigned. \textit  {Top Right)} The distance between labels assigned to images and the location of their true label. This graphic shows that the majority of error was kept within a roughly 10 meter radius. \textit  {Bottom Left)} Another measure of the distance between assigned and true labels in sequential order. From this it may be inferred that the greatest error occurred towards the beginning of the run and at the end. \textit  {Bottom Right)} The same measure as the bottom left, sorted by error magnitude instead. Like the top right, this graphic implies that the majority of error was kept within roughly 10 meters.}}{8}}
\newlabel{error_graphs}{{14}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A confusion matrix showing the correlation between the assigned and true labels in our data set using K-Nearest Neighbors classification.}}{8}}
\newlabel{confusion_matrix}{{15}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Discussion and Future Work}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {VII}conclusion}{8}}
\@writefile{toc}{\contentsline {section}{References}{8}}
\bibcite{Kalman}{6}
\bibcite{AlexNet}{7}
\bibcite{sift}{8}
\bibcite{laser_range}{9}
\bibcite{thrun2002particle}{10}
\bibcite{thrun2002robotic}{11}
\bibcite{valgren2007sift}{12}
\bibcite{cobot}{13}
